{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language modelling using Ngrams\n",
        "\n",
        "## Aim: To build a bigram and trigram model on a corpus and compare the probabilities of models using python and nltk\n",
        "\n"
      ],
      "metadata": {
        "id": "E5E59vmd3iKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install advertools -q"
      ],
      "metadata": {
        "id": "yNILAPr04GzB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710f3f3e-492f-42a5-d4a5-29f407bf94fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.1/310.1 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.9/272.9 KB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions -q"
      ],
      "metadata": {
        "id": "0cJVTOIn_8NW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b6afa17-1808-4703-a9f9-64276ce7c674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.5/104.5 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M9PnTlA23gG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import advertools as adv\n",
        "import re\n",
        "import contractions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thjwl_L14MCF",
        "outputId": "f8072d5a-6384-493a-bd9c-b312d01829c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ngram:\n",
        "\n",
        "  def __init__(self,text):\n",
        "    self.ltext=text.lower()\n",
        "    self.text=contractions.fix(self.ltext)\n",
        "    self.stop_words=set(adv.stopwords['english'])\n",
        "\n",
        "  def remove_quotes_brackets(self):\n",
        "    self.text1=re.sub(\"[\\\"\\'\\(\\)]\", \"\", self.text)\n",
        "    return self.text1\n",
        "\n",
        "  def remove_punctuations(self,text):\n",
        "    text1=re.sub(r\"[^\\w\\s]\", \" \",text)\n",
        "    return text1\n",
        "\n",
        "  def tokenize(self,text):\n",
        "    self.t_text=word_tokenize(text)\n",
        "    return self.t_text\n",
        "\n",
        "  def stop_word(self,text):\n",
        "    self.sw=[]\n",
        "    for i in text:\n",
        "      if i not in self.stop_words:\n",
        "        self.sw.append(i)\n",
        "    return self.sw\n",
        "\n",
        "  def count(self,lst):\n",
        "    d={}\n",
        "    for i in lst:\n",
        "      d[i]=lst.count(i)\n",
        "    return d\n",
        "\n",
        "  def preprocess(self):\n",
        "    q_text=self.remove_quotes_brackets()\n",
        "    p_text=self.remove_punctuations(q_text)\n",
        "    t_text=self.tokenize(p_text)\n",
        "    swtext=self.stop_word(t_text)\n",
        "    word=' '.join(swtext)\n",
        "    ctext=self.count(t_text)\n",
        "    return p_text,t_text,ctext\n",
        "\n",
        "  def n_grams(self,txt,sent,pred,words,n):\n",
        "    sent=sent.lower()\n",
        "    pred=pred.lower()\n",
        "    sent_tup=tuple(sent.split(\" \"))\n",
        "    new=sent+' '+pred\n",
        "    tc=txt.count(new)\n",
        "    if n==2:\n",
        "      b=list(nltk.bigrams(words))\n",
        "      wc=self.count(b)\n",
        "      cnt=wc[sent_tup]\n",
        "      prob1=round(tc/cnt,2)\n",
        "    elif n==3:\n",
        "      t=list(nltk.trigrams(words))\n",
        "      wc1=self.count(t)\n",
        "      cnt1=wc1[sent_tup]\n",
        "      prob1=round(tc/cnt1,2)\n",
        "    return prob1\n",
        "\n",
        "  def model(self):\n",
        "    p_text,t_text,ctext=self.preprocess()\n",
        "    q=input('The sentence : ')\n",
        "    words=q.split(' ')\n",
        "    n=len(words)\n",
        "    # n=2\n",
        "    proba={}\n",
        "    for i in t_text:\n",
        "      prob=self.n_grams(p_text,q,i,t_text,n)\n",
        "      proba[i]=prob\n",
        "    k=list(proba.keys())\n",
        "    v=list(proba.values())\n",
        "    max_v=max(v)\n",
        "    max_k=k[v.index(max(v))]\n",
        "    print(f\"The predicted word after the sentence '{q}' is '{max_k}' with probability '{max_v}'\")"
      ],
      "metadata": {
        "id": "Lc5h1n2o3c6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt='The girl bought a chocolate. The boy ate the chocolate. The girl bought a toy. The girl played with the toy'"
      ],
      "metadata": {
        "id": "QQno2w00_v2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t.model()"
      ],
      "metadata": {
        "id": "UBuVLf3B_xmh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9fd4d80-e047-4129-ad3a-815c38c1961b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence : bought a\n",
            "The predicted word after the sentence 'bought a' is 'chocolate' with probability '0.5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t=ngram(txt)\n",
        "t.model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CPWp2be_xr4",
        "outputId": "842c2242-7d4f-47b7-8d80-ecea5fbe3eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence : girl bought a\n",
            "The predicted word after the sentence 'girl bought a' is 'chocolate' with probability '0.5'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion: The model is able to predict the next words based on bigrams and trigrams correctly."
      ],
      "metadata": {
        "id": "rOLsK68svGbf"
      }
    }
  ]
}