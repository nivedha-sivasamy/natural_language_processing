{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language Model\n",
        "\n",
        "## Aim: To implement language model using LTSM\n",
        "## Dataset: News article"
      ],
      "metadata": {
        "id": "jwvj-CAbqTDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t30Ns7tUh49B",
        "outputId": "53198432-e3a8-4c1a-a75d-bee325daf247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "def dataset_preparation(data):\n",
        "\n",
        "\t# basic cleanup\n",
        "\tcorpus = data.lower().split(\"\\n\")\n",
        "\n",
        "\t# tokenization\n",
        "\ttokenizer.fit_on_texts(corpus)\n",
        "\ttotal_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "\t# create input sequences using list of tokens\n",
        "\tinput_sequences = []\n",
        "\tfor line in corpus:\n",
        "\t\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\t\tfor i in range(1, len(token_list)):\n",
        "\t\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "\t# pad sequences\n",
        "\tmax_sequence_len = max([len(x) for x in input_sequences])\n",
        "\tinput_sequences = np.array(pad_sequences(input_sequences,\n",
        "\t                                         maxlen=max_sequence_len,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t padding='pre'))\n",
        "\n",
        "\t# create predictors and label\n",
        "\tpredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\tlabel = ku.to_categorical(label, num_classes=total_words)\n",
        "\n",
        "\treturn predictors, label, max_sequence_len, total_words\n",
        "\n",
        "def create_model(predictors, label, max_sequence_len, total_words):\n",
        "\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
        "\tmodel.add(LSTM(150, return_sequences = True))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(LSTM(100,return_sequences=False))\n",
        "\tmodel.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam',\n",
        "\t              metrics=['accuracy'])\n",
        "\tearlystop = EarlyStopping(monitor='val_loss', min_delta=0,\n",
        "\t                          patience=5, verbose=0, mode='auto')\n",
        "\tmodel.fit(predictors, label, epochs=100,validation_split=0.2 ,\n",
        "\t          verbose=1,callbacks=[earlystop])\n",
        "\tprint(model.summary())\n",
        "\treturn model\n",
        "\n",
        "def generate_text(seed_text, next_words, max_sequence_len):\n",
        "\tfor _ in range(next_words):\n",
        "\t\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\t\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1,\n",
        "\t\t                           padding='pre')\n",
        "\t\t# predicted = model.predict_classes(token_list, verbose=0)\n",
        "\t\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\t\toutput_word = \"\"\n",
        "\t\tfor word, index in tokenizer.word_index.items():\n",
        "\t\t\tif index == predicted:\n",
        "\t\t\t\toutput_word = word\n",
        "\t\t\t\tbreak\n",
        "\t\tseed_text += \" \" + output_word\n",
        "\treturn seed_text\n"
      ],
      "metadata": {
        "id": "lKA2pFa3p8Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('/content/drive/MyDrive/alice_in_wonderland.txt').read()\n",
        "\n",
        "predictors, label, max_sequence_len, total_words = dataset_preparation(data)\n",
        "model = create_model(predictors, label, max_sequence_len, total_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMvwQynzqgFL",
        "outputId": "d44ab743-cfc0-4f54-eeba-7482611f7174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "627/627 [==============================] - 56s 79ms/step - loss: 6.1740 - accuracy: 0.0525 - val_loss: 6.0191 - val_accuracy: 0.0810\n",
            "Epoch 2/100\n",
            "627/627 [==============================] - 55s 87ms/step - loss: 5.7968 - accuracy: 0.0586 - val_loss: 6.0327 - val_accuracy: 0.0845\n",
            "Epoch 3/100\n",
            "627/627 [==============================] - 56s 89ms/step - loss: 5.6378 - accuracy: 0.0738 - val_loss: 5.9847 - val_accuracy: 0.0891\n",
            "Epoch 4/100\n",
            "627/627 [==============================] - 53s 85ms/step - loss: 5.4912 - accuracy: 0.0778 - val_loss: 5.9613 - val_accuracy: 0.1059\n",
            "Epoch 5/100\n",
            "627/627 [==============================] - 53s 85ms/step - loss: 5.3732 - accuracy: 0.0839 - val_loss: 5.9381 - val_accuracy: 0.1049\n",
            "Epoch 6/100\n",
            "627/627 [==============================] - 54s 87ms/step - loss: 5.2717 - accuracy: 0.0892 - val_loss: 5.9543 - val_accuracy: 0.0993\n",
            "Epoch 7/100\n",
            "627/627 [==============================] - 51s 82ms/step - loss: 5.1893 - accuracy: 0.0935 - val_loss: 5.9828 - val_accuracy: 0.1013\n",
            "Epoch 8/100\n",
            "627/627 [==============================] - 54s 87ms/step - loss: 5.1156 - accuracy: 0.1018 - val_loss: 6.0131 - val_accuracy: 0.1109\n",
            "Epoch 9/100\n",
            "627/627 [==============================] - 54s 86ms/step - loss: 5.0473 - accuracy: 0.1050 - val_loss: 6.0714 - val_accuracy: 0.1089\n",
            "Epoch 10/100\n",
            "627/627 [==============================] - 53s 85ms/step - loss: 4.9861 - accuracy: 0.1104 - val_loss: 6.0830 - val_accuracy: 0.1180\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 15, 10)            26470     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 15, 150)           96600     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 15, 150)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               100400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2647)              267347    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 490,817\n",
            "Trainable params: 490,817\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(\"Alice said\", 10, max_sequence_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXRUYqmWrzRP",
        "outputId": "b38e350a-d4b1-42cf-ea37-52fca66f7ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Alice said to was a little thing ' said the hatter and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion: The model stopped at 10 epochs due to earlystopping. It is predicting the next set of words."
      ],
      "metadata": {
        "id": "ksU4TWNnwIfH"
      }
    }
  ]
}